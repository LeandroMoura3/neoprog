{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruções:\n",
    "\n",
    "Para baixar o conjunto de dados, acessar o seguinte link:\\\n",
    "https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/ \\\n",
    "O conjunto de dados é o 4. Bearing Data Set\\\n",
    "Baixar os conjuntos de dados e descomprimir estas pastas na pasta raiz do github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import os, math, time, random\n",
    "from datetime import datetime\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definições"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFAM:\n",
    "    def _norm(self, X):\n",
    "        result = 0.0\n",
    "        for i in range(self.input_size):\n",
    "            result = + abs(X[i])\n",
    "        return result\n",
    "\n",
    "    def _and_fuzzy_norm(self, X, Y):\n",
    "        AND = []\n",
    "        for i in range(self.input_size):\n",
    "            AND.append(min(X[i], Y[i]))\n",
    "\n",
    "        return self._norm(AND)\n",
    "\n",
    "    def _train(self, X, W):\n",
    "        result = []\n",
    "        for i in range(self.input_size):\n",
    "            result.append(self.beta * min(X[i], W[i]) + (1 - self.beta) * W[i])\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def __init__(self, input_size=6, alpha=1e-5, rho=0.001, beta=1.0, epsilon=0.0, uncommited_value=0.5):\n",
    "        self.rho = rho\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.input_size = 2 * input_size\n",
    "\n",
    "        ZERO = []\n",
    "        M = []\n",
    "\n",
    "        for _ in range(self.input_size):\n",
    "            ZERO.append(0.0)\n",
    "            M.append(0.5)\n",
    "\n",
    "        self.used = []\n",
    "\n",
    "        self.categories = []\n",
    "        self.W = []\n",
    "        self.ZERO = ZERO\n",
    "        self.m = self._norm(M)\n",
    "        self.uncommited_value = uncommited_value\n",
    "\n",
    "    def train(self, X, y):\n",
    "        rho = self.rho\n",
    "        Xc = []\n",
    "        for x in X:\n",
    "            Xc.append(1.0 - x)\n",
    "        X = X.copy()\n",
    "        X.extend(Xc)\n",
    "\n",
    "        t = []\n",
    "        fuzzy_and = []\n",
    "        for i in range(len(self.W)):\n",
    "            fuzzy_and.append(self._and_fuzzy_norm(X, self.W[i]))\n",
    "            t.append(fuzzy_and[i] / (self.alpha + self._norm(self.W[i])))\n",
    "\n",
    "        train_finished = False\n",
    "        stable = True\n",
    "\n",
    "        while not train_finished:\n",
    "            winner = -1\n",
    "            winner_value = -1\n",
    "            for i in range(len(t)):\n",
    "                if t[i] > winner_value:\n",
    "                    winner_value = t[i]\n",
    "                    winner = i\n",
    "\n",
    "            if self.uncommited_value > winner_value:\n",
    "                self.W.append(X)\n",
    "                self.categories.append(y)\n",
    "                self.used.append(False)\n",
    "\n",
    "                train_finished = True\n",
    "                stable = False\n",
    "            else:\n",
    "                fuzzy_and_m = fuzzy_and[winner] / self.m\n",
    "                if fuzzy_and_m > rho:\n",
    "                    if self.categories[winner] == y:\n",
    "                        self.W[winner] = self._train(X, self.W[winner])\n",
    "                        train_finished = True\n",
    "                    else:\n",
    "                        rho = fuzzy_and_m + self.epsilon\n",
    "                        t[winner] = -1\n",
    "                        if (rho >= 1.0):\n",
    "                            train_finished = True\n",
    "                else:\n",
    "                    t[winner] = -1\n",
    "\n",
    "        return stable\n",
    "\n",
    "    def predict(self, X):\n",
    "        Xc = []\n",
    "        for x in X:\n",
    "            Xc.append(1.0 - x)\n",
    "        X = X.copy()\n",
    "        X.extend(Xc)\n",
    "\n",
    "        t = []\n",
    "        fuzzy_and = []\n",
    "        for i in range(len(self.W)):\n",
    "            fuzzy_and.append(self._and_fuzzy_norm(X, self.W[i]))\n",
    "            t.append(fuzzy_and[i] / (self.alpha + self._norm(self.W[i])))\n",
    "\n",
    "        winner = -1\n",
    "        winner_value = -1\n",
    "        for i in range(len(t)):\n",
    "            if t[i] > winner_value:\n",
    "                winner_value = t[i]\n",
    "                winner = i\n",
    "        self.used[winner] = True\n",
    "        return self.categories[winner]\n",
    "\n",
    "    def prune(self):\n",
    "        finished = False\n",
    "        while not finished:\n",
    "            _break = False\n",
    "            for i in range(len(self.W)):\n",
    "                if not self.used[i]:\n",
    "                    del self.W[i]\n",
    "                    del self.categories[i]\n",
    "                    del self.used[i]\n",
    "                    _break = True\n",
    "                    break\n",
    "            if not _break:\n",
    "                finished = True\n",
    "\n",
    "    def print(self):\n",
    "        for i in range(len(self.W)):\n",
    "            print(self.W[i], self.categories[i], self.used[i])\n",
    "            \n",
    "    def shuffle_nasa_train(self, data, init_degeneration=-1, order=2):\n",
    "        X, y = organize_nasa(data, init_degeneration, order)\n",
    "        for i in range(len(X)):\n",
    "            X[i].append(y[i])\n",
    "        stable = True\n",
    "        random.seed(0)\n",
    "        while stable:\n",
    "            random.shuffle(X)\n",
    "            for i in range(len(X)):\n",
    "                result = self.train(X[i][:-1], X[i][-1])\n",
    "                stable = stable and result\n",
    "\n",
    "    def shuffle_nasa_train_check(self, data_train, data_val, init_degeneration=-1, init_degeneration2=-1, order=2):\n",
    "        X, y = organize_nasa(data_train, init_degeneration, order)\n",
    "        for i in range(len(X)):\n",
    "            X[i].append(y[i])\n",
    "        stable = False\n",
    "        random_seed = int(random.random()*1000)\n",
    "        random.seed(random_seed)\n",
    "        count = 0\n",
    "        old = -1\n",
    "        while not stable and count < 15:\n",
    "            stable = True\n",
    "            count += 1\n",
    "            random.shuffle(X)\n",
    "            for i in range(len(X)):\n",
    "                result = self.train(X[i][:-1], X[i][-1])\n",
    "                stable = stable and result\n",
    "            result = self.predict_nasa(data_val, init_degeneration2, order)\n",
    "            print(count, result)\n",
    "            self.prune()\n",
    "            if result == old:\n",
    "                stable = True\n",
    "            old = result\n",
    "        print(count, random_seed)\n",
    "        return result\n",
    "\n",
    "    def train_nasa(self, data, init_degeneration=-1, order=2):\n",
    "        X, y = organize_nasa(data, init_degeneration, order)\n",
    "        stable = True\n",
    "        while stable:\n",
    "            for i in range(len(X)):\n",
    "                result = self.train(X[i], y[i])\n",
    "                stable = stable and result\n",
    "                \n",
    "    def predict_nasa(self, data, init_degeneration=-1, order=2, plot=True):\n",
    "        X, y = organize_nasa(data, init_degeneration, order)\n",
    "        p = []\n",
    "        ok = 0\n",
    "        for i in range(len(X)):\n",
    "            p.append(self.predict(X[i]))\n",
    "            if p[i] == y[i]:\n",
    "                ok += 1\n",
    "            p[-1] *= (100.0 / 6.0)\n",
    "        if plot:\n",
    "            plt.plot(p, 'ro')\n",
    "            plt.plot(smoothing(p))\n",
    "        return ok / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kusaba SFAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum(A,B):\n",
    "    ans=[]\n",
    "    for i in range(len(A)):\n",
    "        #print(\"I and W sizes: \", len(A), len(B))\n",
    "        ans.append(min([A[i],B[i]]))\n",
    "        \n",
    "    ans = np.array(ans)\n",
    "    return ans\n",
    "    \n",
    "def complement_code(X):\n",
    "        I = np.hstack((X, 1-X))\n",
    "        return I\n",
    "    \n",
    "    \n",
    "def one_hot_encode(labels):\n",
    "    print(labels[0])\n",
    "    one_hot_labels = np.zeros((labels.shape[0], 10))\n",
    "    for i in range(labels.shape[0]):\n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "    return one_hot_labels\n",
    "    \n",
    "class simplified_fuzzy_ARTMAP:\n",
    "    \n",
    "    def __init__(self, X_size, label_size, c_max_a, rho_a, rho_ab, alpha=0.00001, beta=1):\n",
    "        self.M_a = X_size    # input vector size\n",
    "        self.M_ab = label_size    # input label vector size\n",
    "        \n",
    "        self.c_max_a = c_max_a # max categories for ART-a\n",
    "        \n",
    "        \n",
    "        self.rho_a = rho_a    # vigilance parameter for ART-a\n",
    "        self.rho_a_baseline = rho_a\n",
    "        self.rho_ab = rho_ab  # vigilance parameter for map field\n",
    "        self.alpha = alpha # choice parameter\n",
    "        self.beta = beta   # learning rate\n",
    "        \n",
    "        self.N_a = 0         # no. of categories of ART_a initialized to zero\n",
    "        \n",
    "        self.W_a = np.ones( (c_max_a, self.M_a*2) ) # initialize W_a with 1s\n",
    "        self.W_ab = np.ones( (self.M_ab, c_max_a) ) # initialize W_ab with 1s\n",
    "        \n",
    "        self.X_ab = np.zeros( (self.M_ab,) )\n",
    "    \n",
    "    def train(self, X, one_hot_labels, rho_a_inc_rate=0.001):\n",
    "        A = complement_code(X)   # shape of X = Mx1, shape of I = 2Mx1\n",
    "        B = one_hot_labels\n",
    "         \n",
    "        self.rho_a =  self.rho_a_baseline\n",
    "         \n",
    "        T = []\n",
    "        for i in range(self.N_a):           \n",
    "            T.append( np.sum(minimum(A,self.W_a[i,:])) / (self.alpha+np.sum(self.W_a[i,:])) ) # calculate output\n",
    "        \n",
    "        J_list = np.argsort(np.array(T))[::-1]  # J_list: indices of F2 nodes with decreasing order of activations        \n",
    "        \n",
    "        for J in J_list:\n",
    "            # Checking for resonance in ART-a  ---\n",
    "            X_a_mod = np.sum(minimum(A,self.W_a[J,:])) \n",
    "                \n",
    "            while X_a_mod >= self.rho_a * np.sum(A): # resonance occured in ART-a \n",
    "                \n",
    "                #####  match tracking  #####\n",
    "                # Checking for resonance in the MAP FIELD  ---\n",
    "                   \n",
    "                self.X_ab_mod = np.sum( minimum( B,self.W_ab[:,J] ) )\n",
    "                \n",
    "                if self.X_ab_mod > self.rho_ab * np.sum(B): # resonance occurs in the MAP FIELD\n",
    "                    # weight update\n",
    "                    self.W_a[J,:] = self.beta*minimum(A,self.W_a[J,:]) + (1-self.beta)*self.W_a[J,:] # weight update of ART-a\n",
    "                    K = np.argmax( B )\n",
    "                    self.W_ab[:,J] = 0\n",
    "                    self.W_ab[K,J] = 1\n",
    "                    return self.W_ab, K\n",
    "                \n",
    "                else: # NO resonance in the MAP FIELD\n",
    "                    self.rho_a += rho_a_inc_rate\n",
    "       \n",
    "        if self.N_a < self.c_max_a:    # no resonance occured in ART-a, create a new category\n",
    "            n = self.N_a\n",
    "            self.W_a[n,:] = self.beta*minimum(A,self.W_a[n,:]) + (1-self.beta)*self.W_a[n,:] # weight update\n",
    "            self.N_a += 1\n",
    "            \n",
    "            K = np.argmax( B )\n",
    "            self.W_ab[:,n] = 0\n",
    "            self.W_ab[K,n] = 1\n",
    "            return self.W_ab, K\n",
    "        \n",
    "        if self.N_a >= self.c_max_a:\n",
    "            print(\"ART-a memory error!\")\n",
    "            return None, None\n",
    "    \n",
    "    def infer(self, X):\n",
    "        A = complement_code(X)   \n",
    "        T = []\n",
    "        for i in range(self.N_a):           \n",
    "            T.append( np.sum(minimum(A,self.W_a[i,:])) / (self.alpha+np.sum(self.W_a[i,:])) ) # calculate output        \n",
    "        #J_list = np.argsort(np.array(T))[::-1]  # J_list: indices of F2 nodes with decreasing order of activations        \n",
    "        #J = J_list[0] # maximum activation\n",
    "        J = np.argmax(np.array(T))\n",
    "        X_ab = self.W_ab[:,J] \n",
    "        return X_ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(y, si = 6):\n",
    "    result = [y[0]]\n",
    "    for i in range(1, len(y)):\n",
    "        rul = y[i]\n",
    "        for _ in range(si):\n",
    "            rul = (result[-1] + rul) / 2.0\n",
    "        result.append(rul)\n",
    "    return result\n",
    "\n",
    "def clip(vector, init=0.0, end=1.0):\n",
    "    vector = vector.copy()\n",
    "    for i in range(len(vector)):\n",
    "        vector[i] = (vector[i] - init) / (end - init)\n",
    "        if vector[i] > 1.0:\n",
    "            vector[i] = 1.0\n",
    "        elif vector[i] < 0.0:\n",
    "            vector[i] = 0.0\n",
    "    return vector\n",
    "\n",
    "def organize_nasa(data, init_degeneration=-1, order=2):\n",
    "    X = []\n",
    "    y = []\n",
    "    len_data = len(data('rms'))\n",
    "    for i in range(order - 1, len_data):\n",
    "        categoric_label = None\n",
    "        if init_degeneration > 0:\n",
    "            new_i = i - init_degeneration\n",
    "            if new_i > 0:\n",
    "                current_label = new_i / (len_data - init_degeneration)\n",
    "            else:\n",
    "                current_label = 0.0\n",
    "        else:\n",
    "            current_label = i / len_data\n",
    "        if current_label < 0.1:\n",
    "            categoric_label = 0\n",
    "        elif current_label < 0.25:\n",
    "            categoric_label = 1\n",
    "        elif current_label < 0.4:\n",
    "            categoric_label = 2\n",
    "        elif current_label < 0.55:\n",
    "            categoric_label = 3\n",
    "        elif current_label < 0.7:\n",
    "            categoric_label = 4\n",
    "        elif current_label < 0.85:\n",
    "            categoric_label = 5\n",
    "        else:\n",
    "            categoric_label = 6\n",
    "        X.append([])\n",
    "        for j in range(order):\n",
    "            X[-1].append(data('rms')[i - j][0])\n",
    "        for j in range(order):\n",
    "            X[-1].append(data('kurtosis')[i - j][0])\n",
    "        for j in range(order):\n",
    "            X[-1].append(data('rmsee')[i - j][0])\n",
    "        y.append(categoric_label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizations factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_normalization(data, key_norm, norm):\n",
    "    data[key_norm] = {}\n",
    "    data[key_norm]['scaler'] = norm\n",
    "    data[key_norm]['data'] = data['data'].reshape((-1, 1))\n",
    "    data[key_norm]['scaler'].fit(data[key_norm]['data'])\n",
    "    data[key_norm]['data'] = data[key_norm]['scaler'].transform(data[key_norm]['data'])\n",
    "\n",
    "def apply_fit_normalization(data, key_norm, scaler):\n",
    "    data[key_norm] = {}\n",
    "    data[key_norm]['data'] = data['data'].reshape((-1, 1))\n",
    "    data[key_norm]['data'] = scaler[key_norm]['scaler'].transform(data[key_norm]['data'])\n",
    "\n",
    "def apply_cumulative_normalization(data, key_norm, new_key_norm, norm):\n",
    "    data[new_key_norm] = {}\n",
    "    data[new_key_norm]['scaler'] = norm\n",
    "    data[new_key_norm]['data'] = data[key_norm]['data'].reshape((-1, 1))\n",
    "    data[new_key_norm]['scaler'].fit(data[new_key_norm]['data'])\n",
    "    data[new_key_norm]['data'] = data[new_key_norm]['scaler'].transform(data[new_key_norm]['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ufrf(t, b, n, y, k):\n",
    "    return y+(k*(b/(n**b))*(t**(b-1)))\n",
    "\n",
    "def my_rms(acc):\n",
    "    return np.sqrt(np.mean(acc**2))\n",
    "\n",
    "def my_kurt(acc):\n",
    "    return st.kurtosis(acc, fisher=False)\n",
    "\n",
    "def extract_data(path, config, step_window=10):\n",
    "    files_list = os.listdir(path)\n",
    "\n",
    "    rms = []\n",
    "    kurtosis = []\n",
    "    for file in files_list:\n",
    "        data = pd.read_csv(path+file, sep='\\t', header=None)\n",
    "        rms.append(my_rms(data))\n",
    "        kurtosis.append(my_kurt(data))\n",
    "\n",
    "    rms = np.array(rms)\n",
    "    kurtosis = np.array(kurtosis)\n",
    "    rms = rms[:, config].copy()\n",
    "    kurtosis = kurtosis[:, config].copy()\n",
    "\n",
    "    partial_rmsee = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for j in range(len(rms) - (step_window - 1)):\n",
    "        step_rmsee = 0.0\n",
    "        for i in range(step_window):\n",
    "            step_rms = rms[j + i]\n",
    "            step_rmsee += -step_rms*math.log(step_rms)\n",
    "        step_rmsee /= step_window\n",
    "        partial_rmsee.append(step_rmsee)\n",
    "\n",
    "    rmsee = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for j in range(len(partial_rmsee) - (step_window - 1)):\n",
    "        step_mean_rmsee = 0.0\n",
    "        for i in range(step_window):\n",
    "            step_mean_rmsee += partial_rmsee[j + i]\n",
    "        step_mean_rmsee /= step_window\n",
    "        rmsee.append(step_mean_rmsee)\n",
    "\n",
    "    return np.array(rms), np.array(kurtosis), np.array(rmsee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraíndo dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "datasets['train'] = {}\n",
    "\n",
    "datasets['train']['rms'] = {}\n",
    "datasets['train']['kurtosis'] = {}\n",
    "datasets['train']['rmsee'] = {}\n",
    "\n",
    "datasets['train']['rms']['data'], datasets['train']['kurtosis']['data'], datasets['train']['rmsee']['data'] = extract_data('./2nd_test/', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = ['rms', 'kurtosis', 'rmsee']\n",
    "\n",
    "datasets['train']['rms']['ufrf'] = {}\n",
    "datasets['train']['kurtosis']['ufrf'] = {}\n",
    "datasets['train']['rmsee']['ufrf'] = {}\n",
    "\n",
    "datasets['train']['rms']['ufrf']['data'] = []\n",
    "datasets['train']['kurtosis']['ufrf']['data'] = []\n",
    "datasets['train']['rmsee']['ufrf']['data'] = []\n",
    "\n",
    "for i in range(len(datasets['train']['rms']['data'])):\n",
    "    datasets['train']['rms']['ufrf']['data'].append(ufrf(i, 12.0917, 281.0209, 0.0773, 1.38e-5))\n",
    "    datasets['train']['kurtosis']['ufrf']['data'].append(ufrf(i, 11.8280, 86.7537,  3.4471, 3.8e-10))\n",
    "    datasets['train']['rmsee']['ufrf']['data'].append(ufrf(i, 5.0,  100.0, 0.19796, 3.55e-4))\n",
    "    \n",
    "datasets['train']['rms']['ufrf']['data'] = np.array(datasets['train']['rms']['ufrf']['data'])\n",
    "datasets['train']['kurtosis']['ufrf']['data'] = np.array(datasets['train']['kurtosis']['ufrf']['data'])\n",
    "datasets['train']['rmsee']['ufrf']['data'] = np.array(datasets['train']['rmsee']['ufrf']['data'])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=((16, 3)))\n",
    "\n",
    "for i in range(len(datas)):\n",
    "    axs[i].plot(datasets['train'][datas[i]]['data'])\n",
    "    axs[i].plot(datasets['train'][datas[i]]['ufrf']['data'])\n",
    "    axs[i].set_title(datas[i] + ' train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizações dos dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "keyed_scalers = [('MaxAbs', MaxAbsScaler), ('MinMax', MinMaxScaler), ('Standard', StandardScaler), ('Robust', RobustScaler)]\n",
    "scalers = ['MaxAbs', 'MinMax', 'Standard', 'Robust']\n",
    "\n",
    "for scaler_key, scaler in keyed_scalers:\n",
    "    for key in datas:\n",
    "        apply_normalization(datasets['train'][key], scaler_key, scaler())\n",
    "        apply_fit_normalization(datasets['train'][key]['ufrf'], scaler_key, datasets['train'][key])\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=((16, 16)))\n",
    "for i in range(len(datas)):\n",
    "    for j in range(len(scalers)):\n",
    "        axs[j][i].plot(datasets['train'][datas[i]][scalers[j]]['data'])\n",
    "        axs[j][i].plot(datasets['train'][datas[i]]['ufrf'][scalers[j]]['data'])\n",
    "        axs[j][i].set_title(scalers[j] + ' ' + datas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=((16, 3)))\n",
    "\n",
    "for i in range(len(scalers)):\n",
    "    axs[i].plot(datasets['train']['rms']['ufrf'][scalers[i]]['data'])\n",
    "    axs[i].plot(datasets['train']['kurtosis']['ufrf'][scalers[i]]['data'])\n",
    "    axs[i].plot(datasets['train']['rmsee']['ufrf'][scalers[i]]['data'])\n",
    "    axs[i].set_title('ufrf cliped' + scalers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entre 0.0 e 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaler_key in scalers:\n",
    "    for key in datas:\n",
    "        datasets['train'][key][scaler_key + '_Cliped'] = {}\n",
    "        datasets['train'][key][scaler_key + '_Cliped']['data'] = clip(datasets['train'][key][scaler_key]['data'])\n",
    "        datasets['train'][key]['ufrf'][scaler_key + '_Cliped'] = {}\n",
    "        datasets['train'][key]['ufrf'][scaler_key + '_Cliped']['data'] = clip(datasets['train'][key]['ufrf'][scaler_key]['data'])\n",
    "        \n",
    "fig, axs = plt.subplots(4, 3, figsize=((16, 16)))\n",
    "for i in range(len(datas)):\n",
    "    for j in range(len(scalers)):\n",
    "        axs[j][i].plot(datasets['train'][datas[i]][scalers[j] + '_Cliped']['data'])\n",
    "        axs[j][i].plot(datasets['train'][datas[i]]['ufrf'][scalers[j] + '_Cliped']['data'])\n",
    "        axs[j][i].set_title(scalers[j] + ' cliped ' + datas[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=((16, 3)))\n",
    "\n",
    "for i in range(len(scalers)):\n",
    "    axs[i].plot(datasets['train']['rms']['ufrf'][scalers[i] + '_Cliped']['data'])\n",
    "    axs[i].plot(datasets['train']['kurtosis']['ufrf'][scalers[i] + '_Cliped']['data'])\n",
    "    axs[i].plot(datasets['train']['rmsee']['ufrf'][scalers[i] + '_Cliped']['data'])\n",
    "    axs[i].set_title('ufrf cliped' + scalers[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaler_key in scalers:\n",
    "    for key in datas:\n",
    "        datasets['train'][key][scaler_key + '_Zoom'] = {}\n",
    "        datasets['train'][key][scaler_key + '_Zoom']['data'] = clip(datasets['train'][key][scaler_key]['data'], init = 0.5 if key == 'rmsee' else 0.05, end = 1.0 if key == 'rmsee' else 0.3)\n",
    "        datasets['train'][key]['ufrf'][scaler_key + '_Zoom'] = {}\n",
    "        datasets['train'][key]['ufrf'][scaler_key + '_Zoom']['data'] = clip(datasets['train'][key]['ufrf'][scaler_key]['data'], init = 0.5 if key == 'rmsee' else 0.05, end = 1.0 if key == 'rmsee' else 0.3)\n",
    "        \n",
    "fig, axs = plt.subplots(4, 3, figsize=((16, 16)))\n",
    "for i in range(len(datas)):\n",
    "    for j in range(len(scalers)):\n",
    "        axs[j][i].plot(datasets['train'][datas[i]][scalers[j] + '_Zoom']['data'])\n",
    "        axs[j][i].plot(datasets['train'][datas[i]]['ufrf'][scalers[j] + '_Zoom']['data'])\n",
    "        axs[j][i].set_title(scalers[j] + ' zoom ' + datas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo dados de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datasets['test'] = {}\n",
    "\n",
    "datasets['test']['rms'] = {}\n",
    "datasets['test']['kurtosis'] = {}\n",
    "datasets['test']['rmsee'] = {}\n",
    "\n",
    "datasets['test']['rms']['data'], datasets['test']['kurtosis']['data'], datasets['test']['rmsee']['data'] = extract_data('./1st_test/', 4)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=((16, 3)))\n",
    "\n",
    "for i in range(len(datas)):\n",
    "    axs[i].plot(datasets['test'][datas[i]]['data'])\n",
    "    axs[i].set_title('test ' + datas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização dos dados de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaler_key, scaler in keyed_scalers:\n",
    "    for key in datas:\n",
    "        apply_fit_normalization(datasets['test'][key], scaler_key, datasets['train'][key])\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=((16, 16)))\n",
    "for i in range(len(datas)):\n",
    "    for j in range(len(scalers)):\n",
    "        axs[j][i].plot(datasets['test'][datas[i]][scalers[j]]['data'])\n",
    "        axs[j][i].set_title(scalers[j] + ' ' + datas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entre 0.0 e 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaler_key in scalers:\n",
    "    for key in datas:\n",
    "        datasets['test'][key][scaler_key + '_Cliped'] = {}\n",
    "        datasets['test'][key][scaler_key + '_Cliped']['data'] = clip(datasets['test'][key][scaler_key]['data'])\n",
    "        \n",
    "fig, axs = plt.subplots(4, 3, figsize=((16, 16)))\n",
    "for i in range(len(datas)):\n",
    "    for j in range(len(scalers)):\n",
    "        axs[j][i].plot(datasets['test'][datas[i]][scalers[j] + '_Cliped']['data'])\n",
    "        axs[j][i].set_title(scalers[j] + ' cliped ' + datas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaler_key in scalers:\n",
    "    for key in datas:\n",
    "        datasets['test'][key][scaler_key + '_Zoom'] = {}\n",
    "        datasets['test'][key][scaler_key + '_Zoom']['data'] = clip(datasets['test'][key][scaler_key]['data'], init = 0.5 if key == 'rmsee' else 0.05, end = 1.0 if key == 'rmsee' else 0.3)\n",
    "        \n",
    "fig, axs = plt.subplots(4, 3, figsize=((16, 16)))\n",
    "for i in range(len(datas)):\n",
    "    for j in range(len(scalers)):\n",
    "        axs[j][i].plot(datasets['test'][datas[i]][scalers[j] + '_Zoom']['data'])\n",
    "        axs[j][i].set_title(scalers[j] + ' zoom ' + datas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralizando o padrão de operação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['train']['rms']['MaxAbs_Center'] = {}\n",
    "datasets['train']['rms']['MaxAbs_Center']['data'] = clip(datasets['train']['rms']['MaxAbs']['data'] + 0.4)\n",
    "datasets['train']['kurtosis']['MaxAbs_Center'] = {}\n",
    "datasets['train']['kurtosis']['MaxAbs_Center']['data'] = clip(datasets['train']['kurtosis']['MaxAbs']['data'] + 0.3)\n",
    "datasets['train']['rmsee']['MaxAbs_Center'] = {}\n",
    "datasets['train']['rmsee']['MaxAbs_Center']['data'] = clip(datasets['train']['rmsee']['MaxAbs']['data'] - 0.05)\n",
    "datasets['train']['rms']['ufrf']['MaxAbs_Center'] = {}\n",
    "datasets['train']['rms']['ufrf']['MaxAbs_Center']['data'] = clip(datasets['train']['rms']['ufrf']['MaxAbs']['data'] + 0.4)\n",
    "datasets['train']['kurtosis']['ufrf']['MaxAbs_Center'] = {}\n",
    "datasets['train']['kurtosis']['ufrf']['MaxAbs_Center']['data'] = clip(datasets['train']['kurtosis']['ufrf']['MaxAbs']['data'] + 0.3)\n",
    "datasets['train']['rmsee']['ufrf']['MaxAbs_Center'] = {}\n",
    "datasets['train']['rmsee']['ufrf']['MaxAbs_Center']['data'] = clip(datasets['train']['rmsee']['ufrf']['MaxAbs']['data'] - 0.05)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=((16, 4)))\n",
    "for i in range(len(datas)):\n",
    "    for j in range(len(['MaxAbs_Center'])):\n",
    "        axs[i].plot(datasets['train'][datas[i]]['MaxAbs_Center']['data'])\n",
    "        axs[i].plot(datasets['train'][datas[i]]['ufrf']['MaxAbs_Center']['data'])\n",
    "        line = []\n",
    "        for _ in range(len(datasets['train'][datas[i]]['MaxAbs_Center']['data'])):\n",
    "            line.append(0.5)\n",
    "        axs[i].plot(line)\n",
    "        axs[i].set_title('MaxAbs_Center' + ' ' + datas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['test']['rms']['MaxAbs_Center'] = {}\n",
    "datasets['test']['rms']['MaxAbs_Center']['data'] = clip(datasets['test']['rms']['MaxAbs']['data'] + 0.275)\n",
    "datasets['test']['kurtosis']['MaxAbs_Center'] = {}\n",
    "datasets['test']['kurtosis']['MaxAbs_Center']['data'] = clip(datasets['test']['kurtosis']['MaxAbs']['data'] + 0.295)\n",
    "datasets['test']['rmsee']['MaxAbs_Center'] = {}\n",
    "datasets['test']['rmsee']['MaxAbs_Center']['data'] = clip(datasets['test']['rmsee']['MaxAbs']['data'] - 0.325)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=((16, 4)))\n",
    "for i in range(len(datas)):\n",
    "    for j in range(len(['MaxAbs_Center'])):\n",
    "        axs[i].plot(datasets['test'][datas[i]]['MaxAbs_Center']['data'])\n",
    "        line = []\n",
    "        for _ in range(len(datasets['test'][datas[i]]['MaxAbs_Center']['data'])):\n",
    "            line.append(0.5)\n",
    "        axs[i].plot(line)\n",
    "        axs[i].set_title('MaxAbs_Center' + ' ' + datas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo dados de validação 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['test2'] = {}\n",
    "\n",
    "datasets['test2']['rms'] = {}\n",
    "datasets['test2']['kurtosis'] = {}\n",
    "datasets['test2']['rmsee'] = {}\n",
    "\n",
    "datasets['test2']['rms']['data'], datasets['test2']['kurtosis']['data'], datasets['test2']['rmsee']['data'] = extract_data('./1st_test/', 6)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=((16, 3)))\n",
    "\n",
    "for i in range(len(datas)):\n",
    "    axs[i].plot(datasets['test2'][datas[i]]['data'])\n",
    "    axs[i].set_title('test2 ' + datas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizando dados de validação 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaler_key, scaler in keyed_scalers:\n",
    "    for key in datas:\n",
    "        apply_fit_normalization(datasets['test2'][key], scaler_key, datasets['train'][key])\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=((16, 16)))\n",
    "for i in range(len(datas)):\n",
    "    for j in range(len(scalers)):\n",
    "        axs[j][i].plot(datasets['test2'][datas[i]][scalers[j]]['data'])\n",
    "        axs[j][i].set_title(scalers[j] + ' ' + datas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entre 0.0 e 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scaler_key in scalers:\n",
    "    for key in datas:\n",
    "        datasets['test2'][key][scaler_key + '_Cliped'] = {}\n",
    "        datasets['test2'][key][scaler_key + '_Cliped']['data'] = clip(datasets['test2'][key][scaler_key]['data'])\n",
    "        \n",
    "fig, axs = plt.subplots(4, 3, figsize=((16, 16)))\n",
    "for i in range(len(datas)):\n",
    "    for j in range(len(scalers)):\n",
    "        axs[j][i].plot(datasets['test2'][datas[i]][scalers[j] + '_Cliped']['data'])\n",
    "        axs[j][i].set_title(scalers[j] + ' cliped ' + datas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFAM - MaxAbs_Clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+1, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-18, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 1e-18, 1e+0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-18, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 1e-18, 1.0)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     sfam.train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # #Trial 112 finished with value: 0.3947100712105799 and parameters: {'rho': 3.2358034120332154e-05, 'uv': 0.9835112751325361, 'alpha': 2.7270781618220184e-06}. Best is trial 112 with value: 0.3947100712105799.\n",
    "# # #Trial 188 finished with value: 0.40183112919633773 and parameters: {'rho': 0.9295486941424583, 'uv': 0.984398658226122, 'alpha': 2.2668089565117578e-07, 'beta': 0.9426549691910234}. Best is trial 188 with value: 0.40183112919633773.\n",
    "# # #Trial 108 finished with value: 0.427263479145473 and parameters: {'rho': 0.0009686028049355828, 'uv': 0.9883119821455072, 'alpha': 4.084979094859232e-07, 'beta': 6.613883552322061e-05}. Best is trial 108 with value: 0.427263479145473.\n",
    "# # #Trial 871 finished with value: 0.427263479145473 and parameters: {'alpha': 1.1167576673504459e-08, 'rho': 0.0005592497019541445, 'beta': 0.005437777540556062, 'uv': 0.989645864609353}. Best is trial 871 with value: 0.427263479145473.\n",
    "# # #Best is trial 241 with value: 0.42929806714140384. {'alpha': 3.395245641548146e-11, 'rho': 4.3521380132425176e-08, 'beta': 2.3557187791365324e-07, 'uv': 0.9885496137738817}\n",
    "# # #Best is trial 890 with value: 0.6866734486266531. {'alpha': 0.08704579858923452, 'rho': 1.5842446614486082e-12, 'beta': 0.02433635084039448, 'uv': 0.9480472449474258}\n",
    "# # #Best is trial 164 with value: 0.7293997965412004. {'alpha': 0.45977710064840743, 'rho': 4.592646278009706e-13, 'beta': 6.62470116222985e-11, 'uv': 6.140198929780164e-11}\n",
    "# # # Best is trial 106 with value: 0.7426246185147508. {'alpha': 1.2517591322643648e-16, 'rho': 1.5164290602975099e-15, 'beta': 0.03496236267182363, 'uv': 0.5249178974615589}\n",
    "# # sfam['MaxAbs'] = SFAM(alpha = 4.6715861565370895e-14, rho = 3.4726756552458553e-10, beta = 4.688230591139349e-17, uncommited_value = 0.9739101209573398)\n",
    "# # {'alpha': 2.617325177043259e-06, 'rho': 7.679082091677435e-12, 'beta': 0.035495950650972226, 'uv': 0.6868487573343112}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfam['MaxAbs'] = SFAM(alpha = 3.3864038844388143e-06, rho = 3.657477124222113e-05, beta = 0.04844502173902665, epsilon = 0.23474188550938033, uncommited_value = 0.28034744230011377)\n",
    "\n",
    "sfam['MaxAbs'].train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392)\n",
    "sfam['MaxAbs'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs']['data'], 392)\n",
    "sfam['MaxAbs'].prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs'].predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - MaxAbs_Clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     n_layers = trial.suggest_int('n_layers', 1, 6)\n",
    "#     layers = []\n",
    "#     for i in range(n_layers):\n",
    "#         layers.append(trial.suggest_int(f'n_units_{i}', 1, 200))\n",
    "\n",
    "#     activation = trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu'])\n",
    "#     solver = trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam'])\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+0, log=True)\n",
    "    \n",
    "#     if solver == 'sgd':\n",
    "#         learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'invscaling', 'adaptive'])\n",
    "#         momentum = trial.suggest_float(\"momentum\", 1e-18, 1e+0, log=True)\n",
    "#     else:\n",
    "#         learning_rate = 'constant'\n",
    "#         momentum = 0.9\n",
    "        \n",
    "#     if solver == 'adam':\n",
    "#         beta_1 = trial.suggest_float(\"beta_1\", 1e-18, 1e+0, log=True)\n",
    "#         beta_2 = trial.suggest_float(\"beta_2\", 1e-18, 1e+0, log=True)\n",
    "#     else:\n",
    "#         beta_1 = 0.9\n",
    "#         beta_2 = 0.999\n",
    "    \n",
    "    \n",
    "#     clf = MLPClassifier(hidden_layer_sizes=tuple(layers), activation = activation, solver=solver, beta_1 = beta_1, beta_2 = beta_2, learning_rate = learning_rate, momentum = momentum, random_state = 0)\n",
    "#     x_train, y_train = organize_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392)\n",
    "#     x_val, y_val = organize_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'])\n",
    "    \n",
    "#     clf.fit(x_train, y_train)\n",
    "#     return clf.score(x_val, y_val)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "# #Best is trial 141 with value: 0.7568667344862665. {'n_layers': 2, 'n_units_0': 1, 'n_units_1': 87, 'activation': 'tanh', 'solver': 'lbfgs', 'alpha': 6.800537599344594e-16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = {}\n",
    "mlp['MaxAbs'] = MLPClassifier(hidden_layer_sizes=(186, 42, 193, 189, 135, 145), activation = 'identity', solver='adam', alpha = 1.2049123869661849e-12, beta_1= 8.660310409635591e-05, beta_2= 0.013736105687331128, random_state = 0)\n",
    "\n",
    "x_train, y_train = organize_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392)\n",
    "x_val, y_val = organize_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)\n",
    "mlp['MaxAbs'].fit(x_train, y_train)\n",
    "\n",
    "p = np.array(mlp['MaxAbs'].predict(x_val)) * (100.0 / 6.0)\n",
    "\n",
    "plt.plot(p, 'ro')\n",
    "plt.plot(smoothing(p))\n",
    "mlp['MaxAbs'].score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = organize_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'])\n",
    "p = np.array(mlp['MaxAbs'].predict(x_test)) * (100.0 / 6.0)\n",
    "\n",
    "plt.plot(p, 'ro')\n",
    "plt.plot(smoothing(p))\n",
    "mlp['MaxAbs'].score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFAM - MaxABs_Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+1, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-18, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 1e-18, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.5, 1e+0, log=True)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, uncommited_value = uncommited_value)\n",
    "#     sfam.train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Center']['data'], 392)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Center']['data'], 392, False)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=250)\n",
    "# print(study.best_trial.params)\n",
    "# # Best is trial 616 with value: 0.4008138351983723. {'alpha': 1.481329512950453e-10, 'rho': 3.693479376150351e-12, 'beta': 4.149916915933312e-09, 'uv': 0.9848333333556476}\n",
    "# # Best is trial 466 with value: 0.7039674465920651. {'alpha': 5.622048313518151e-13, 'rho': 8.151965761257136e-09, 'beta': 5.763877184354314e-16, 'uv': 0.9924770110389107}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Center'] = SFAM(alpha = 1.2932349835971422e-15, rho = 5.112297845240373e-12, beta = 0.036769828903903894, uncommited_value = 0.7552986535083299)\n",
    "\n",
    "sfam['MaxAbs_Center'].train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Center']['data'], 392)\n",
    "sfam['MaxAbs_Center'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Center']['data'], 392)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Center'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Center']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - MaxAbs_Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     n_layers = trial.suggest_int('n_layers', 1, 6)\n",
    "#     layers = []\n",
    "#     for i in range(n_layers):\n",
    "#         layers.append(trial.suggest_int(f'n_units_{i}', 1, 200))\n",
    "\n",
    "#     activation = trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu'])\n",
    "#     solver = trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam'])\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+0, log=True)\n",
    "    \n",
    "#     if solver == 'sgd':\n",
    "#         learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'invscaling', 'adaptive'])\n",
    "#         momentum = trial.suggest_float(\"momentum\", 1e-18, 1e+0, log=True)\n",
    "#     else:\n",
    "#         learning_rate = 'constant'\n",
    "#         momentum = 0.9\n",
    "        \n",
    "#     if solver == 'adam':\n",
    "#         beta_1 = trial.suggest_float(\"beta_1\", 1e-18, 1e+0, log=True)\n",
    "#         beta_2 = trial.suggest_float(\"beta_2\", 1e-18, 1e+0, log=True)\n",
    "#     else:\n",
    "#         beta_1 = 0.9\n",
    "#         beta_2 = 0.999\n",
    "    \n",
    "    \n",
    "#     clf = MLPClassifier(hidden_layer_sizes=tuple(layers), activation = activation, solver=solver, beta_1 = beta_1, beta_2 = beta_2, learning_rate = learning_rate, momentum = momentum, random_state = 0)\n",
    "#     x_train, y_train = organize_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Center']['data'], 392)\n",
    "#     x_val, y_val = organize_nasa(lambda x: datasets['train'][x]['MaxAbs_Center']['data'], 392)\n",
    "    \n",
    "#     clf.fit(x_train, y_train)\n",
    "#     return clf.score(x_val, y_val)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=200)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # #mlp['MaxAbs_Center'] = MLPClassifier(hidden_layer_sizes=(141, 84, 95, 22, 104), activation = 'relu', solver='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = {}\n",
    "mlp['MaxAbs_Center'] = MLPClassifier(hidden_layer_sizes=(110, 90, 92, 170, 66), activation = 'identity', solver='lbfgs', alpha = 7.793279570750885e-10, random_state = 0)\n",
    "\n",
    "x_train, y_train = organize_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Center']['data'], 392)\n",
    "x_val, y_val = organize_nasa(lambda x: datasets['train'][x]['MaxAbs_Center']['data'], 392)\n",
    "mlp['MaxAbs_Center'].fit(x_train, y_train)\n",
    "\n",
    "p = np.array(mlp['MaxAbs_Center'].predict(x_val)) * (100.0 / 6.0)\n",
    "\n",
    "plt.plot(p, 'ro')\n",
    "plt.plot(smoothing(p))\n",
    "mlp['MaxAbs_Center'].score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = organize_nasa(lambda x: datasets['test'][x]['MaxAbs_Center']['data'])\n",
    "p = np.array(mlp['MaxAbs_Center'].predict(x_test)) * (100.0 / 6.0)\n",
    "\n",
    "plt.plot(p, 'ro')\n",
    "plt.plot(smoothing(p))\n",
    "mlp['MaxAbs_Center'].score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFAM - MaxAbs_Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+1, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-18, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 1e-18, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.5, 1.0, log=True)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, uncommited_value = uncommited_value)\n",
    "#     sfam.train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Zoom']['data'], 392)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Zoom']['data'], 392)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # Best is trial 106 with value: 0.7426246185147508. {'alpha': 8.711044148724726e-15, 'rho': 1.651959726556269e-05, 'beta': 0.03556285811793728, 'uv': 0.622653730160301}\n",
    "# # 0.7507629704984741 SFAM(alpha = 7.614130239297938e-16, rho = 0.0368073300339664, beta = 0.018306735816381994, uncommited_value = 0.860340588623127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Zoom'] = SFAM(alpha = 1.1661840026962726, rho = 2.7052884043004445e-05, beta = 0.8725909192315182, uncommited_value = 0.8909178357025973)\n",
    "\n",
    "sfam['MaxAbs_Zoom'].train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Zoom']['data'], 392)\n",
    "sfam['MaxAbs_Zoom'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Zoom']['data'], 392)\n",
    "sfam['MaxAbs_Zoom'].prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Zoom'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Zoom']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - MaxAbs_Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     n_layers = trial.suggest_int('n_layers', 1, 6)\n",
    "#     layers = []\n",
    "#     for i in range(n_layers):\n",
    "#         layers.append(trial.suggest_int(f'n_units_{i}', 1, 200))\n",
    "\n",
    "#     activation = trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu'])\n",
    "#     solver = trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam'])\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+0, log=True)\n",
    "    \n",
    "#     if solver == 'sgd':\n",
    "#         learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'invscaling', 'adaptive'])\n",
    "#         momentum = trial.suggest_float(\"momentum\", 1e-18, 1e+0, log=True)\n",
    "#     else:\n",
    "#         learning_rate = 'constant'\n",
    "#         momentum = 0.9\n",
    "        \n",
    "#     if solver == 'adam':\n",
    "#         beta_1 = trial.suggest_float(\"beta_1\", 1e-18, 1e+0, log=True)\n",
    "#         beta_2 = trial.suggest_float(\"beta_2\", 1e-18, 1e+0, log=True)\n",
    "#     else:\n",
    "#         beta_1 = 0.9\n",
    "#         beta_2 = 0.999\n",
    "    \n",
    "    \n",
    "#     clf = MLPClassifier(hidden_layer_sizes=tuple(layers), activation = activation, solver=solver, beta_1 = beta_1, beta_2 = beta_2, learning_rate = learning_rate, momentum = momentum, random_state = 0)\n",
    "#     x_train, y_train = organize_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Zoom']['data'], 392)\n",
    "#     x_val, y_val = organize_nasa(lambda x: datasets['train'][x]['MaxAbs_Zoom']['data'], 392)\n",
    "    \n",
    "#     clf.fit(x_train, y_train)\n",
    "#     return clf.score(x_val, y_val)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=200)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # #mlp['MaxAbs_Center'] = MLPClassifier(hidden_layer_sizes=(141, 84, 95, 22, 104), activation = 'relu', solver='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp['MaxAbs_Zoom'] = MLPClassifier(hidden_layer_sizes=(79, 124), activation = 'relu', solver='adam', alpha = 2.9734674750533706e-15, beta_1 = 2.092544907459088e-09, beta_2 = 0.4640112349610577, random_state = 0)\n",
    "\n",
    "x_train, y_train = organize_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Zoom']['data'], 392)\n",
    "x_val, y_val = organize_nasa(lambda x: datasets['train'][x]['MaxAbs_Zoom']['data'], 392)\n",
    "mlp['MaxAbs_Zoom'].fit(x_train, y_train)\n",
    "\n",
    "p = np.array(mlp['MaxAbs_Zoom'].predict(x_val)) * (100.0 / 6.0)\n",
    "\n",
    "plt.plot(p, 'ro')\n",
    "plt.plot(smoothing(p))\n",
    "mlp['MaxAbs_Zoom'].score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = organize_nasa(lambda x: datasets['test'][x]['MaxAbs_Zoom']['data'])\n",
    "p = np.array(mlp['MaxAbs_Zoom'].predict(x_test)) * (100.0 / 6.0)\n",
    "\n",
    "plt.plot(p, 'ro')\n",
    "plt.plot(smoothing(p))\n",
    "mlp['MaxAbs_Zoom'].score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino variando ordem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFAM - MaxAbs_Cliped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+1, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-18, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 1e-18, 1e+0, log=True)\n",
    "#     order = trial.suggest_int(\"order\", 3, 25)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-18, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 1e-18, 1.0)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     sfam.train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392, order)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "# #Best is trial 56 with value: 0.7370030581039755. {'alpha': 1.901072353223224e-09, 'rho': 0.09435357495043345, 'beta': 0.04970296173592995, 'order': 4, 'uv': 0.8271269742914835}\n",
    "# # order = 13\n",
    "# # sfam['MaxAbs_HOrder'] = SFAM(input_size=order * 3, alpha = 1.4937614035813953e-05, rho = 0.001602074207017527, beta = 0.09486927599859632, uncommited_value = 0.6703351329846337)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 4\n",
    "sfam['MaxAbs_HOrder'] = SFAM(input_size=order * 3, alpha = 5.7178955012023405, rho = 0.1313726446633941, beta = 2.9058567705878517e-10, epsilon = 3.0314685749891967e-14, uncommited_value = 0.30476807052343236)\n",
    "\n",
    "sfam['MaxAbs_HOrder'].train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392, order)\n",
    "sfam['MaxAbs_HOrder'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)\n",
    "sfam['MaxAbs_HOrder'].prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_HOrder'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFAM - MaxAbs_Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+1, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-18, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 1e-18, 1e+0, log=True)\n",
    "#     order = trial.suggest_int(\"order\", 3, 25)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.5, 1.0, log=True)\n",
    "    \n",
    "#     sfam = SFAM(input_size=3 * order, alpha = alpha, rho = rho, beta = beta, uncommited_value = uncommited_value)\n",
    "#     sfam.train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Center']['data'], 392, order)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Center']['data'], 392, order)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "# #Best is trial 89 with value: 0.7320328542094456. {'alpha': 3.664125890863213e-10, 'rho': 5.385562525240191e-13, 'beta': 0.06498813095645183, 'order': 11, 'uv': 0.7742260125177193}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 11\n",
    "sfam['MaxAbs_Center_HOrder'] = SFAM(input_size=order * 3, alpha = 3.664125890863213e-10, rho = 5.385562525240191e-13, beta = 0.06498813095645183, uncommited_value = 0.7742260125177193)\n",
    "\n",
    "sfam['MaxAbs_Center_HOrder'].train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Center']['data'], 392, order)\n",
    "sfam['MaxAbs_Center_HOrder'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Center']['data'], 392, order)\n",
    "sfam['MaxAbs_Center_HOrder'].prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Center_HOrder'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Center']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFAM - MaxAbs_Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+1, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-18, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 1e-18, 1e+0, log=True)\n",
    "#     order = trial.suggest_int(\"order\", 3, 25)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.5, 1.0, log=True)\n",
    "    \n",
    "#     sfam = SFAM(input_size=3 * order, alpha = alpha, rho = rho, beta = beta, uncommited_value = uncommited_value)\n",
    "#     sfam.train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Zoom']['data'], 392, order)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Zoom']['data'], 392, order)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 14\n",
    "sfam['MaxAbs_Zoom_HOrder'] = SFAM(input_size=order * 3, alpha = 0.49315860824457886, rho = 1.0329266002189683e-16, beta = 7.128032283404492e-14, uncommited_value = 0.5723724108474563)\n",
    "\n",
    "sfam['MaxAbs_Zoom_HOrder'].train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Zoom']['data'], 392, order)\n",
    "sfam['MaxAbs_Zoom_HOrder'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Zoom']['data'], 392, order)\n",
    "sfam['MaxAbs_Zoom_HOrder'].prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Zoom_HOrder'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Zoom']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+1, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-18, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 1e-18, 1e+0, log=True)\n",
    "#     order = trial.suggest_int(\"order\", 3, 25)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-18, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 1e-18, 1.0)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     sfam.train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392, order)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 6\n",
    "sfam['MaxAbs_Zoom_HOrder'] = SFAM(input_size=order * 3, alpha = 1.706915146560203e-11, rho = 1.9858072835258157e-15, beta = 6.885308818236919e-13, epsilon = 6.226374702800287e-13, uncommited_value = 0.4122217880215322)\n",
    "\n",
    "sfam['MaxAbs_Zoom_HOrder'].train_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392, order)\n",
    "sfam['MaxAbs_Zoom_HOrder'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)\n",
    "sfam['MaxAbs_Zoom_HOrder'].prune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+1, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-18, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 1e-18, 1e+0, log=True)\n",
    "#     order = trial.suggest_int(\"order\", 3, 25)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.5, 1.0, log=True)\n",
    "    \n",
    "#     sfam = SFAM(input_size=3 * order, alpha = alpha, rho = rho, beta = beta, uncommited_value = uncommited_value)\n",
    "#     sfam.train_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     n_layers = trial.suggest_int('n_layers', 1, 6)\n",
    "#     layers = []\n",
    "#     for i in range(n_layers):\n",
    "#         layers.append(trial.suggest_int(f'n_units_{i}', 1, 200))\n",
    "\n",
    "#     activation = trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu'])\n",
    "#     solver = trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam'])\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-18, 1e+0, log=True)\n",
    "    \n",
    "#     if solver == 'sgd':\n",
    "#         learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'invscaling', 'adaptive'])\n",
    "#         momentum = trial.suggest_float(\"momentum\", 1e-18, 1e+0, log=True)\n",
    "#     else:\n",
    "#         learning_rate = 'constant'\n",
    "#         momentum = 0.9\n",
    "        \n",
    "#     if solver == 'adam':\n",
    "#         beta_1 = trial.suggest_float(\"beta_1\", 1e-18, 1e+0, log=True)\n",
    "#         beta_2 = trial.suggest_float(\"beta_2\", 1e-18, 1e+0, log=True)\n",
    "#     else:\n",
    "#         beta_1 = 0.9\n",
    "#         beta_2 = 0.999\n",
    "    \n",
    "#     order = trial.suggest_int(\"order\", 3, 25)\n",
    "    \n",
    "#     clf = MLPClassifier(hidden_layer_sizes=tuple(layers), activation = activation, solver=solver, beta_1 = beta_1, beta_2 = beta_2, learning_rate = learning_rate, momentum = momentum, random_state = 0)\n",
    "#     x_train, y_train = organize_nasa(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Zoom']['data'], 392, order)\n",
    "#     x_val, y_val = organize_nasa(lambda x: datasets['train'][x]['MaxAbs_Zoom']['data'], 392, order)\n",
    "    \n",
    "#     clf.fit(x_train, y_train)\n",
    "#     return clf.score(x_val, y_val)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # #mlp['MaxAbs_Center'] = MLPClassifier(hidden_layer_sizes=(141, 84, 95, 22, 104), activation = 'relu', solver='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sem order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com ufrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.011518807525753528, rho = 8.515579176690106e-09, beta = 0.006392025284083809, epsilon = 3.374690966331042e-06, uncommited_value = 0.4331646065996603)\n",
    "# # Best is trial 14 with value: 0.7080366225839267. {'alpha': 1.3218366955128671e-10, 'rho': 0.00020931919708588607, 'beta': 0.5048218866249008, 'epsilon': 2.557039916965787e-07, 'uv': 0.3262950183849552}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.08561132960605239, rho = 0.016531520983938345, beta = 0.713474522053423, epsilon = 0.021119268522436573, uncommited_value = 0.8625161376591538)\n",
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 1.8816157592091586e-08, rho = 1.8617039027712794e-08, beta = 0.5004790482050023, epsilon = 2.2637857050173263e-05, uncommited_value = 0.6723310783439534)\n",
    "\n",
    "sfam['MaxAbs_Shuffle'] = SFAM(alpha = 1.3218366955128671e-10, rho = 0.00020931919708588607, beta = 0.5048218866249008, epsilon = 2.557039916965787e-07, uncommited_value = 0.3262950183849552)\n",
    "\n",
    "sfam['MaxAbs_Shuffle'].shuffle_nasa_train(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392)\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].prune()\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem ufrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.011518807525753528, rho = 8.515579176690106e-09, beta = 0.006392025284083809, epsilon = 3.374690966331042e-06, uncommited_value = 0.4331646065996603)\n",
    "# # Best is trial 0 with value: 0.9989827060020345. {'alpha': 124.33591160816164, 'rho': 0.8954900706107042, 'beta': 0.6483389611000279, 'epsilon': 3.373420294938334e-05, 'uv': 0.7933369764989281}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.011518807525753528, rho = 8.515579176690106e-09, beta = 0.006392025284083809, epsilon = 3.374690966331042e-06, uncommited_value = 0.4331646065996603)\n",
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 574613.1582716915, rho = 0.06582664439658847, beta = 0.9588551126247702, epsilon = 0.583275640426613, uncommited_value = 0.06723002127854139)\n",
    "\n",
    "sfam['MaxAbs_Shuffle'] = SFAM(alpha = 124.33591160816164, rho = 0.8954900706107042, beta = 0.6483389611000279, epsilon = 3.373420294938334e-05, uncommited_value = 0.7933369764989281)\n",
    "\n",
    "sfam['MaxAbs_Shuffle'].shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].prune()\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No teste com ufrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'])\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.011518807525753528, rho = 8.515579176690106e-09, beta = 0.006392025284083809, epsilon = 3.374690966331042e-06, uncommited_value = 0.4331646065996603)\n",
    "# # Best is trial 57 with value: 0.1791183294663573. {'alpha': 2.5503821359530783e-10, 'rho': 3.794072947394362e-10, 'beta': 0.5035388417799306, 'epsilon': 6.703404444159925e-08, 'uv': 0.03233433981105656}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 3.1939107658466404e-05, rho = 1.4421999746657232e-07, beta = 0.502077534680409, epsilon = 0.053559564843432994, uncommited_value = 0.3820435792972554)\n",
    "\n",
    "sfam['MaxAbs_Shuffle'] = SFAM(alpha = 2.5503821359530783e-10, rho = 3.794072947394362e-10, beta = 0.5035388417799306, epsilon = 6.703404444159925e-08, uncommited_value = 0.03233433981105656)\n",
    "\n",
    "sfam['MaxAbs_Shuffle'].shuffle_nasa_train(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392)\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].prune()\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No teste sem ufrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'])\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.011518807525753528, rho = 8.515579176690106e-09, beta = 0.006392025284083809, epsilon = 3.374690966331042e-06, uncommited_value = 0.4331646065996603)\n",
    "# # Best is trial 2 with value: 0.29466357308584684. {'alpha': 0.2415900242307281, 'rho': 2.988578392335302e-07, 'beta': 0.5823068087626335, 'epsilon': 6.518734598615543e-07, 'uv': 0.0437451658545504}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 2.980088610848902e-09, rho = 2.39144697622943e-10, beta = 0.5078115199710304, epsilon = 0.7372350083061249, uncommited_value = 0.8923221469596608)\n",
    "sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.2415900242307281, rho = 2.988578392335302e-07, beta = 0.5823068087626335, epsilon = 6.518734598615543e-07, uncommited_value = 0.0437451658545504)\n",
    "\n",
    "sfam['MaxAbs_Shuffle'].shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].prune()\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Com order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com ufrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "#     order = trial.suggest_int(\"order\", 3, 20)\n",
    "    \n",
    "#     sfam = SFAM(input_size = order * 3, alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     return sfam.shuffle_nasa_train_check(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, 392, order)\n",
    "# #     sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392, order)\n",
    "# #     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.011518807525753528, rho = 8.515579176690106e-09, beta = 0.006392025284083809, epsilon = 3.374690966331042e-06, uncommited_value = 0.4331646065996603)\n",
    "# # Best is trial 27 with value: 0.6931702344546381. {'alpha': 149.71079710310886, 'rho': 1.9584913510588663e-06, 'beta': 0.7523023832761473, 'epsilon': 1.0409789466973102e-10, 'uv': 0.7372130690308065, 'order': 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 1.6567020207018912, rho = 0.037607532992751984, beta = 0.7324093401615666, epsilon = 0.0007816538021598521, uncommited_value = 0.8279089243935547)\n",
    "# order = 4\n",
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 469485651.63208663, rho = 2.6688818345839734e-05, beta = 0.5900653734072322, epsilon = 1.0428311616811367e-08, uncommited_value = 0.6765298014258014)\n",
    "\n",
    "order = 16\n",
    "sfam['MaxAbs_Shuffle'] = SFAM(alpha = 5.101005052299004e-07, rho = 2.9019542435177472e-06, beta = 0.5300322471842625, epsilon = 1.0093954849744568e-09, uncommited_value = 0.4448266074469005)\n",
    "\n",
    "sfam.shuffle_nasa_train_check(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, 392, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].prune()\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem ufrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "#     order = trial.suggest_int(\"order\", 3, 20)\n",
    "    \n",
    "#     sfam = SFAM(input_size = order * 3, alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     return sfam.shuffle_nasa_train_check(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, 392, order)\n",
    "# #     sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)\n",
    "# #     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.011518807525753528, rho = 8.515579176690106e-09, beta = 0.006392025284083809, epsilon = 3.374690966331042e-06, uncommited_value = 0.4331646065996603)\n",
    "# # Best is trial 1 with value: 1.0. {'alpha': 101418685.07021816, 'rho': 0.2022551607325245, 'beta': 0.7899503948439658, 'epsilon': 2.2343956586156464e-05, 'uv': 0.4418993084323386, 'order': "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 1.6567020207018912, rho = 0.037607532992751984, beta = 0.7324093401615666, epsilon = 0.0007816538021598521, uncommited_value = 0.8279089243935547)\n",
    "# order = 17\n",
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 776290.5535842044, rho = 0.0008149210583998265, beta = 0.5504618504091746, epsilon = 4.898721222384571e-07, uncommited_value = 0.2670182720631446)\n",
    "\n",
    "order = 6\n",
    "sfam['MaxAbs_Shuffle'] = SFAM(alpha = 101418685.07021816, rho = 0.2022551607325245, beta = 0.7899503948439658, epsilon = 2.2343956586156464e-05, uncommited_value = 0.4418993084323386)\n",
    "\n",
    "sfam['MaxAbs_Shuffle'].shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].prune()\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No teste com ufrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "#     order = trial.suggest_int(\"order\", 3, 20)\n",
    "   \n",
    "\n",
    "#     sfam = SFAM(input_size = order * 3, alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     return sfam.shuffle_nasa_train_check(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], 392, -1, order)\n",
    "\n",
    "# #     sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392, order)\n",
    "# #     return sfam.predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # Best is trial 11 with value: 0.320596458527493. {'alpha': 4.8179476843870175, 'rho': 0.006300934375598044, 'beta': 0.8746908353941755, 'epsilon': 0.007416403995602103, 'uv': 0.6137230906894608, 'order': 11}\n",
    "# # Best is trial 32 with value: 0.3196644920782852. {'alpha': 3.8849255702993912, 'rho': 0.25171772646372714, 'beta': 0.9802369004005982, 'epsilon': 0.003223583859375817, 'uv': 0.9031201293244469, 'order': 11}\n",
    "# #  Trial 1 finished with value: 0.3091247672253259 and parameters: {'alpha': 0.6990187053883001, 'rho': 0.08047084425919833, 'beta': 0.7423731624116697, 'epsilon': 0.21014919701609575, 'uv': 0.8298748035701702, 'order': 9}. Best is trial 1 with value: 0.3091247672253259.\n",
    "# # Best is trial 99 with value: 0.3201304753028891. {'alpha': 4.146533021444089, 'rho': 0.34780385570064076, 'beta': 0.7381454273349647, 'epsilon': 0.0012169857086828419, 'uv': 0.6485861269372781, 'order': 11}\n",
    "# # Best is trial 94 with value: 0.3358139534883721. {'alpha': 0.08658189560125516, 'rho': 0.05655521476588831, 'beta': 0.6226900538809652, 'epsilon': 0.22454196787560562, 'uv': 0.25833754745440723, 'order': 7}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order = 11\n",
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 3.8849255702993912, rho = 0.25171772646372714, beta = 0.9802369004005982, epsilon = 0.007416403995602103, uncommited_value = 0.6137230906894608)\n",
    "# order = 11\n",
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 4.8179476843870175, rho = 0.006300934375598044, beta = 0.8746908353941755, epsilon = 0.003223583859375817, uncommited_value = 0.9031201293244469)\n",
    "# order = 7\n",
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 32098.74662404218, rho = 0.04079029146066474, beta = 0.8443649735126442, epsilon = 9.657077310824467e-07, uncommited_value = 0.3676079007440835)\n",
    "\n",
    "order = 7\n",
    "sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.08658189560125516, rho = 0.05655521476588831, beta = 0.6226900538809652, epsilon = 0.22454196787560562, uncommited_value = 0.25833754745440723)\n",
    "\n",
    "sfam['MaxAbs_Shuffle'].shuffle_nasa_train(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Cliped']['data'], 392, order)\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].prune()\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No teste sem ufrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "    rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "    beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "    uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "    order = trial.suggest_int(\"order\", 3, 20)\n",
    "    \n",
    "    sfam = SFAM(input_size = order * 3, alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "    return sfam.shuffle_nasa_train_check(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], 392, -1, order)\n",
    "\n",
    "#     sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial.params)\n",
    "\n",
    "# Best is trial 35 with value: 0.28637842863784285. {'alpha': 9.494078495031685, 'rho': 0.00026585600668354616, 'beta': 0.9262820746486281, 'epsilon': 0.00011060820265777134, 'uv': 0.6910187069818342, 'order': 6}\n",
    "# Best is trial 82 with value: 0.32199440820130476. {'alpha': 102104946173.51483, 'rho': 2.771555395966763e-10, 'beta': 0.51774676508801, 'epsilon': 2.119348068873209e-05, 'uv': 0.7910702188267866, 'order': 11}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order = 10\n",
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 14324.463391596633, rho = 2.8120435320894876e-10, beta = 0.7192702154560318, epsilon = 0.30250629155823466, uncommited_value = 0.9634119255832562)\n",
    "order = 11\n",
    "sfam['MaxAbs_Shuffle'] = SFAM(alpha = 102104946173.51483, rho = 2.771555395966763e-10, beta = 0.51774676508801, epsilon = 2.119348068873209e-05, uncommited_value = 0.7910702188267866)\n",
    "\n",
    "sfam['MaxAbs_Shuffle'].shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].prune()\n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "#     order = trial.suggest_int(\"order\", 3, 20)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['ufrf']['MaxAbs_Center']['data'], 392, order)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['train'][x]['MaxAbs_Center']['data'], 392, order)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # Best is trial 35 with value: 0.28637842863784285. {'alpha': 9.494078495031685, 'rho': 0.00026585600668354616, 'beta': 0.9262820746486281, 'epsilon': 0.00011060820265777134, 'uv': 0.6910187069818342, 'order': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 11\n",
    "sfam['MaxAbs_Shuffle'] = SFAM(alpha = 1.6567020207018912, rho = 0.037607532992751984, beta = 0.7324093401615666, epsilon = 0.0007816538021598521, uncommited_value = 0.8279089243935547)\n",
    "\n",
    "for i in range(10):\n",
    "    sfam['MaxAbs_Shuffle'].shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392, order)\n",
    "    print(i, sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order))\n",
    "    \n",
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfam['MaxAbs_Shuffle'].predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'], -1, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "    rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "    beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "    uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "    \n",
    "    sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "    sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)\n",
    "    return sfam.predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data']) + sfam.predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'])\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial.params)\n",
    "\n",
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.011518807525753528, rho = 8.515579176690106e-09, beta = 0.006392025284083809, epsilon = 3.374690966331042e-06, uncommited_value = 0.4331646065996603)\n",
    "# Best is trial 2 with value: 0.29466357308584684. {'alpha': 0.2415900242307281, 'rho': 2.988578392335302e-07, 'beta': 0.5823068087626335, 'epsilon': 6.518734598615543e-07, 'uv': 0.0437451658545504}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-10, 1e+12, log=True)\n",
    "    rho = trial.suggest_float(\"rho\", 1e-10, 1e+0, log=True)\n",
    "    beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "    epsilon = trial.suggest_float(\"epsilon\", 1e-10, 1e+0, log=True)\n",
    "    uncommited_value = trial.suggest_float(\"uv\", 0.001, 1e+0)\n",
    "    \n",
    "    sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "    sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Cliped']['data'], 392)\n",
    "    return sfam.predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Cliped']['data']) + sfam.predict_nasa(lambda x: datasets['test2'][x]['MaxAbs_Cliped']['data'])\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial.params)\n",
    "\n",
    "# sfam['MaxAbs_Shuffle'] = SFAM(alpha = 0.011518807525753528, rho = 8.515579176690106e-09, beta = 0.006392025284083809, epsilon = 3.374690966331042e-06, uncommited_value = 0.4331646065996603)\n",
    "# Best is trial 2 with value: 0.29466357308584684. {'alpha': 0.2415900242307281, 'rho': 2.988578392335302e-07, 'beta': 0.5823068087626335, 'epsilon': 6.518734598615543e-07, 'uv': 0.0437451658545504}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando novos parâmetros para Weibull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 1e-4, 1e+1, log=True)\n",
    "#     rho = trial.suggest_float(\"rho\", 1e-4, 1e+0, log=True)\n",
    "#     beta = trial.suggest_float(\"beta\", 0.5, 1e+0, log=True)\n",
    "#     epsilon = trial.suggest_float(\"epsilon\", 1e-4, 1e+0, log=True)\n",
    "#     uncommited_value = trial.suggest_float(\"uv\", 0.5, 1e+0)\n",
    "#     order = trial.suggest_int(\"order\", 3, 20)\n",
    "    \n",
    "#     sfam = SFAM(alpha = alpha, rho = rho, beta = beta, epsilon = epsilon, uncommited_value = uncommited_value)\n",
    "#     sfam.shuffle_nasa_train(lambda x: datasets['train'][x]['MaxAbs_Center']['data'], 392, order)\n",
    "#     return sfam.predict_nasa(lambda x: datasets['test'][x]['MaxAbs_Center']['data'], -1, order)\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=50)\n",
    "# print(study.best_trial.params)\n",
    "\n",
    "# # Best is trial 35 with value: 0.28637842863784285. {'alpha': 9.494078495031685, 'rho': 0.00026585600668354616, 'beta': 0.9262820746486281, 'epsilon': 0.00011060820265777134, 'uv': 0.6910187069818342, 'order': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     b = trial.suggest_float(\"b\", 1e-5, 1e+5, log=True)\n",
    "#     n = trial.suggest_float(\"n\", 1e-5, 1e+5, log=True)\n",
    "#     y = trial.suggest_float(\"y\", 1e-5, 1e+5, log=True)\n",
    "#     k = trial.suggest_float(\"k\", 1e-5, 1e+5, log=True)\n",
    "\n",
    "#     error = 0\n",
    "#     try:\n",
    "#         for i in range(len(datasets['train']['rms']['data'])):\n",
    "#             error += (datasets['train']['rms']['data'][i] - ufrf(i, b, n, y, k))**2\n",
    "#     except BaseException as err:\n",
    "#         print(err)\n",
    "#         return float('inf')\n",
    "#     return error\n",
    "                  \n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=10000)\n",
    "# print(study.best_trial.params)\n",
    "# # for i in range(len(datasets['train']['rms']['data'])):\n",
    "# #     datasets['train']['rms']['ufrf']['data'].append(ufrf(i, 12.0917, 281.0209, 0.0773, 1.38e-5))\n",
    "# #     datasets['train']['kurtosis']['ufrf']['data'].append(ufrf(i, 11.8280, 86.7537,  3.4471, 3.8e-10))\n",
    "# #     datasets['train']['rmsee']['ufrf']['data'].append(ufrf(i, 5.0,  100.0, 0.19796, 3.55e-4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     b = trial.suggest_float(\"b\", 1e-10, 1e+10, log=True)\n",
    "#     n = trial.suggest_float(\"n\", 1e-10, 1e+10, log=True)\n",
    "#     y = trial.suggest_float(\"y\", 1e-10, 1e+10, log=True)\n",
    "#     k = trial.suggest_float(\"k\", 1e-10, 1e+10, log=True)\n",
    "\n",
    "#     error = 0\n",
    "#     try:\n",
    "#         for i in range(len(datasets['train']['kurtosis']['data'])):\n",
    "#             error += (datasets['train']['kurtosis']['data'][i] - ufrf(i, b, n, y, k))**2\n",
    "#     except BaseException as err:\n",
    "#         print(err)\n",
    "#         return float('inf')\n",
    "#     return error\n",
    "                  \n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=12000)\n",
    "# print(study.best_trial.params)\n",
    "# # for i in range(len(datasets['train']['rms']['data'])):\n",
    "# #     datasets['train']['rms']['ufrf']['data'].append(ufrf(i, 12.0917, 281.0209, 0.0773, 1.38e-5))\n",
    "# #     datasets['train']['kurtosis']['ufrf']['data'].append(ufrf(i, 11.8280, 86.7537,  3.4471, 3.8e-10))\n",
    "# #     datasets['train']['rmsee']['ufrf']['data'].append(ufrf(i, 5.0,  100.0, 0.19796, 3.55e-4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     b = trial.suggest_float(\"b\", 1e-10, 1e+10, log=True)\n",
    "#     n = trial.suggest_float(\"n\", 1e-10, 1e+10, log=True)\n",
    "#     y = trial.suggest_float(\"y\", 1e-10, 1e+10, log=True)\n",
    "#     k = trial.suggest_float(\"k\", 1e-10, 1e+10, log=True)\n",
    "\n",
    "#     error = 0\n",
    "#     try:\n",
    "#         for i in range(len(datasets['train']['rmsee']['data'])):\n",
    "#             error += (datasets['train']['rmsee']['data'][i] - ufrf(i, b, n, y, k))**2\n",
    "#     except BaseException as err:\n",
    "#         print(err)\n",
    "#         return float('inf')\n",
    "#     return error\n",
    "                  \n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=12000)\n",
    "# print(study.best_trial.params)\n",
    "# # for i in range(len(datasets['train']['rms']['data'])):\n",
    "# #     datasets['train']['rms']['ufrf']['data'].append(ufrf(i, 12.0917, 281.0209, 0.0773, 1.38e-5))\n",
    "# #     datasets['train']['kurtosis']['ufrf']['data'].append(ufrf(i, 11.8280, 86.7537,  3.4471, 3.8e-10))\n",
    "# #     datasets['train']['rmsee']['ufrf']['data'].append(ufrf(i, 5.0,  100.0, 0.19796, 3.55e-4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     b = trial.suggest_float(\"b\", 1e-10, 1e+10, log=True)\n",
    "#     n = trial.suggest_float(\"n\", 1e-10, 1e+10, log=True)\n",
    "#     y = trial.suggest_float(\"y\", 1e-10, 1e+10, log=True)\n",
    "#     k = trial.suggest_float(\"k\", 1e-10, 1e+10, log=True)\n",
    "\n",
    "#     error = 0\n",
    "#     try:\n",
    "#         for i in range(len(datasets['train']['rms']['data'])):\n",
    "#             error += (datasets['train']['rms']['data'][i] - ufrf(i, b, n, y, k))**2\n",
    "#     except BaseException as err:\n",
    "#         print(err)\n",
    "#         return float('inf')\n",
    "#     return error\n",
    "                  \n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=12000)\n",
    "# print(study.best_trial.params)\n",
    "# # for i in range(len(datasets['train']['rms']['data'])):\n",
    "# #     datasets['train']['rms']['ufrf']['data'].append(ufrf(i, 12.0917, 281.0209, 0.0773, 1.38e-5))\n",
    "# #     datasets['train']['kurtosis']['ufrf']['data'].append(ufrf(i, 11.8280, 86.7537,  3.4471, 3.8e-10))\n",
    "# #     datasets['train']['rmsee']['ufrf']['data'].append(ufrf(i, 5.0,  100.0, 0.19796, 3.55e-4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# import optuna\n",
    "\n",
    "# def objective(trial):\n",
    "#     b = trial.suggest_float(\"b\", 1e-10, 1e+10, log=True)\n",
    "#     n = trial.suggest_float(\"n\", 1e-10, 1e+10, log=True)\n",
    "#     y = trial.suggest_float(\"y\", 1e-10, 1e+10, log=True)\n",
    "#     k = trial.suggest_float(\"k\", 1e-10, 1e+10, log=True)\n",
    "\n",
    "#     error = 0\n",
    "#     try:\n",
    "#         for i in range(len(datasets['train']['kurtosis']['data'])):\n",
    "#             error += (datasets['train']['kurtosis']['data'][i] - ufrf(i, b, n, y, k))**2\n",
    "#     except BaseException as err:\n",
    "#         print(err)\n",
    "#         return float('inf')\n",
    "#     return error\n",
    "                  \n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=25000)\n",
    "# print(study.best_trial.params)\n",
    "# # for i in range(len(datasets['train']['rms']['data'])):\n",
    "# #     datasets['train']['rms']['ufrf']['data'].append(ufrf(i, 12.0917, 281.0209, 0.0773, 1.38e-5))\n",
    "# #     datasets['train']['kurtosis']['ufrf']['data'].append(ufrf(i, 11.8280, 86.7537,  3.4471, 3.8e-10))\n",
    "# #     datasets['train']['rmsee']['ufrf']['data'].append(ufrf(i, 5.0,  100.0, 0.19796, 3.55e-4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 13109 finished with value: 651.8197403775264 and parameters: {'b': 3.1647763222847693, 'n': 9655.298502441472, 'y': 3.3565863046236797, 'k': 686924.7810499861}. Best is trial 13109 with value: 651.8197403775264."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets['train']['rms']['ufrf2'] = {}\n",
    "# datasets['train']['kurtosis']['ufrf2'] = {}\n",
    "# datasets['train']['rmsee']['ufrf2'] = {}\n",
    "\n",
    "# datasets['train']['rms']['ufrf2']['data'] = []\n",
    "# datasets['train']['kurtosis']['ufrf2']['data'] = []\n",
    "# datasets['train']['rmsee']['ufrf2']['data'] = []\n",
    "\n",
    "# for i in range(len(datasets['train']['rms']['data'])):\n",
    "#     datasets['train']['rms']['ufrf2']['data'].append(ufrf(i, 14.229038186252977, 668.782319256043, 0.08808774870823785, 0.06837028155861612))\n",
    "#     datasets['train']['kurtosis']['ufrf2']['data'].append(ufrf(i, 3.1647763222847693, 8965.298502441472, 3.3565863046236797, 686924.7810499861))\n",
    "#     datasets['train']['rmsee']['ufrf2']['data'].append(ufrf(i, 1.9853779542341599,  111560.79648079904, 0.15513975802213667, 904791.9996814801))\n",
    "    \n",
    "# datasets['train']['rms']['ufrf2']['data'] = np.array(datasets['train']['rms']['ufrf2']['data'])\n",
    "# datasets['train']['kurtosis']['ufrf2']['data'] = np.array(datasets['train']['kurtosis']['ufrf2']['data'])\n",
    "# datasets['train']['rmsee']['ufrf2']['data'] = np.array(datasets['train']['rmsee']['ufrf2']['data'])\n",
    "\n",
    "# fig, axs = plt.subplots(1, 3, figsize=((16, 3)))\n",
    "\n",
    "# for i in range(len(datas)):\n",
    "#     axs[i].plot(datasets['train'][datas[i]]['data'])\n",
    "#     axs[i].plot(datasets['train'][datas[i]]['ufrf2']['data'])\n",
    "#     axs[i].set_title(datas[i] + ' train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ufrf(t, b, n, y, k):\n",
    "#     return y+(k*(b/(n**b))*(t**(b-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
